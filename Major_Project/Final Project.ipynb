{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verzeo - Major Project (ML-JULY-B1)\n",
    "\n",
    "### Done by : Sanjay Marreddi  \n",
    "### Email Id  : sanjay.marreddi.19041@iitgoa.ac.in\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let us import the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score,classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us read the given Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = pd.read_excel('information.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "information.isnull().sum() # Checking the Missing Values in each Column of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HaetMap of the Correlation Matrix\n",
    "sns.heatmap(information.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information =  information[[\"gender\", \"name\",\"description\", \"gender:confidence\",\"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Dropping the rows with NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = information.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning of \"description\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stopwords = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only those rows which has type of description str. \n",
    "temp=[]\n",
    "for i in range(information.shape[0]):\n",
    "    temp.append(type(information['description'].values[i]) == str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = information.loc[temp,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n",
    "# Data Cleaning using regex.\n",
    "information['cleaned_description'] = information['description'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning of \"text\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n",
    "information['cleaned_text'] = information['text'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only those rows with full confidence in gender\n",
    "information = information[information[\"gender:confidence\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_information =  information[[\"name\",\"cleaned_text\",\"cleaned_description\"]]\n",
    "\n",
    "y_information = information[[\"gender\"]] # Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_information.shape)\n",
    "X_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_information.shape)\n",
    "y_information.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Label Encoding the gender column for ease of Calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_information['gender_encoded'] = le.fit_transform(y_information[\"gender\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_information = y_information[[\"gender_encoded\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_information.shape)\n",
    "X_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_information.shape)\n",
    "y_information.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Machine Learning Modelling\n",
    "\n",
    "## 1. Classification using Naive Bayes Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_information.shape)\n",
    "X_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_information.shape)\n",
    "y_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X_Nb, Test_X_Nb, Train_Y_Nb, Test_Y_Nb = train_test_split(X_information['cleaned_description'],y_information['gender_encoded'],test_size = 0.01, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing Label Encoding for both Training and Testing Sets.\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y_Nb = Encoder.fit_transform(Train_Y_Nb)\n",
    "Test_Y_Nb = Encoder.fit_transform(Test_Y_Nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming text to feature vectors that can be used as input to estimator.\n",
    "T_vect = TfidfVectorizer(max_features=5000)\n",
    "T_vect.fit(X_information['cleaned_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the Train and Test sets.\n",
    "Train_X_Nb_T = T_vect.transform(Train_X_Nb)\n",
    "Test_X_Nb_T = T_vect.transform(Test_X_Nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the NB classifier.\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Nb_T,Train_Y_Nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on validation dataset.\n",
    "predictions_Nb = Naive.predict(Test_X_Nb_T)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy.\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_Nb, Test_Y_Nb)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification using Support Vector Machine Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_information.shape)\n",
    "X_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (y_information.shape)\n",
    "y_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X_Svm, Test_X_Svm, Train_Y_Svm, Test_Y_Svm = train_test_split(X_information['cleaned_description'],y_information['gender_encoded'],test_size = 0.01, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing Label Encoding for both Training and Testing Sets.\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y_Svm = Encoder.fit_transform(Train_Y_Svm)\n",
    "Test_Y_Svm = Encoder.fit_transform(Test_Y_Svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming text to feature vectors that can be used as input to estimator.\n",
    "T_vect = TfidfVectorizer(max_features=5000)\n",
    "T_vect.fit(X_information['cleaned_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the Train and Test sets.\n",
    "Train_X_Svm_T = T_vect.transform(Train_X_Svm)\n",
    "Test_X_Svm_T = T_vect.transform(Test_X_Svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=2.0, kernel='linear', degree=2, gamma='auto')\n",
    "SVM.fit(Train_X_Svm_T,Train_Y_Svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on validation dataset\n",
    "predictions_Svm = SVM.predict(Test_X_Svm_T)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_Svm, Test_Y_Svm)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification using Logistic Regression Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_information.shape)\n",
    "X_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_information.shape)\n",
    "y_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X_Lr, Test_X_Lr, Train_Y_Lr, Test_Y_Lr = train_test_split(X_information['cleaned_description'],y_information['gender_encoded'],test_size = 0.01, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing Label Encoding for both Training and Testing Sets.\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y_Lr = Encoder.fit_transform(Train_Y_Lr)\n",
    "Test_Y_Lr = Encoder.fit_transform(Test_Y_Lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming text to feature vectors that can be used as input to estimator.\n",
    "T_vect = TfidfVectorizer(max_features=5000)\n",
    "T_vect.fit(X_information['cleaned_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the Train and Test sets.\n",
    "Train_X_Lr_T = T_vect.transform(Train_X_Lr)\n",
    "Test_X_Lr_T = T_vect.transform(Test_X_Lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the classifier\n",
    "lr = LogisticRegression(max_iter = 1000)\n",
    "lr.fit(Train_X_Lr_T,Train_Y_Lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on validation dataset\n",
    "predictions_Lr = lr.predict(Test_X_Lr_T)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"LR Accuracy Score -> \",accuracy_score(predictions_Lr, Test_Y_Lr)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Machine Learning Modelling -> Choosing best Algo based on Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From above Models, Accuracy obtained through\n",
    "\n",
    "1. Naive Bayes Algorithm           is **71.42857142857143 %**\n",
    "\n",
    "2. Support Vector Machine Algorithm is **68.0672268907563 %**\n",
    "\n",
    "3. Logistic Regression Algorithm is  **70.58823529411765 %**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### So, For the given data set and the used hyperparameters, Based on the accuracy score , We can say **Naive Bias Algorithm is best** in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions on DataSet \n",
    "\n",
    "###### 1.What are the most common emotions/words used by Males and Females?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = pd.read_excel('information.xlsx')\n",
    "\n",
    "information =  information[[\"gender\", \"name\",\"description\", \"gender:confidence\",\"text\"]]\n",
    "\n",
    "information = information.dropna()\n",
    "\n",
    "# Taking only those rows which has type of description str. \n",
    "temp=[]\n",
    "for i in range(information.shape[0]):\n",
    "    temp.append(type(information['description'].values[i]) == str)\n",
    "    \n",
    "information = information.loc[temp,:]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n",
    "# Data Cleaning using regex.\n",
    "information['cleaned_description'] = information['description'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n",
    "information['cleaned_text'] = information['text'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "\n",
    "information = information[information[\"gender:confidence\"]==1]\n",
    "\n",
    "\n",
    "X_information =  information[[\"gender\", \"name\",\"cleaned_text\",\"cleaned_description\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_information.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_info = X_information.loc[ (X_information.gender == \"female\" ) | (X_information.gender == \"male\" ) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
